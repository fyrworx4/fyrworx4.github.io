---
title: Red vs. Blue - Networking Explained
layout: post
date: 2022-09-27 22:01
image: /assets/images/markdown.jpg
headerImage: false
tag:
- rvb
category: blog
author: taylornguyen
description: The final boss of Red vs. BLue
---

Hello! Welcome back to my third installment of Red vs. Blue content. This post will aim to explain the concepts and developments of the networking infrastructure behind our Red vs. Blue project.

---

### Quick refresher on Red vs. Blue

Red vs. Blue, or RvB, is our college organization's twist on the CCDC format. Teams of 4 are placed into a vulnerable environment and given a unique experience of troubleshooting, responding to incidents, and completing business objectives against an active red team.

Preparation and development of Red vs. Blue competition involves few major steps: brainstorming the setting, creating vulnerable machines, configuring the networking, and setting up a scoring engine.

To read more about RvB, check out this blog by [CPP SWIFT](https://www.calpolyswift.org/blog/rvb-2022-spring).

This is going to be an update to [my first RvB post](https://tsnguyen.com/red-vs-blue/), so please check my previous post out if you haven't already.

### Previous Red vs. Blue networking configuration

This was how RvB was previously setup. After the previous networking guy graduated, this was basically what I was left with in terms of documentation.

![Topology](../assets/images/red-vs-blue/RvB-Topology.png)

### Primer to ESXi and vSphere networking

We run RvB mostly off ESXi, which is a Type-1 hypervisor made by VMware. In addition to creating VMs, you can create virtual networks to connect VMs together.

- VMware ESXi allows you to create **vSwitches**, or virtual switches.
- vSwitches operate pretty much exactly the same as physical switches.
- vSwitches contain **port groups**, or groups of virtual ports on a vSwitch.

To understand port groups a bit better, imagine you have a super huge, unconfigured virtual switch that contains an absurdly large number of ports.

Chances are, you don't want all the ports to be configured exactly the same way -- you want some ports to be used by Production VMs, some ports to be used by Development VMs, some ports to be used by Engineering VMs, etc.

We can make a port group for each of these:
- a new port group for all production VMs called "Production Port Group",
- a new port group for all development VMs called "Development Port Group",
- a new port group for all engineering VMs called "Engineering Port Group",
- and so on and so forth.

We can then create logical rules or policies on each port group. For example, if we want to implement traffic shaping on our production VMs, we can do so by editing the traffic policies on the production VMs' port group without affecting the development VMs and engineering VMs.

ESXi port groups are a totally different concept than things like port bonding, port aggregation, etc. If you want to learn more, [this article](https://vmiss.net/vmware-vsphere-networking/) has a good read on vSphere networking.

### 1-to-1 NAT

The first thing I focused on was reimplementing the 1-to-1 NAT on the pod pfSense routers. I did not know how to configure 1-to-1 NAT, so I had to consult the [pfSense documentation](https://docs.netgate.com/pfsense/en/latest/nat/1-1.html) to figure out what was going on.

The goal of 1-to-1 NAT is to get every system on each pod to be individually accessible from the core network (AKA have its own IP address in the core network subnet range, which was 172.16.0.0/16).

![1-to-1 NAT](../assets/images/red-vs-blue/rvb-networking-nat.png)

After much trial and error, this was the NAT configuration that allowed us to achieve 1-to-1 NAT:

- Under System → Routing, ensure that the default gateway is configured to be the Core Router's LAN interface (172.16.0.1).
- Under Firewall → Virtual IPs, create a new entry with the following settings:
  - Type - Proxy ARP
  - Interface - WAN
  - Address Type - Network
  - Address(es) - 172.16.X.0/24 (X being the team number)
- Under Firewall → NAT, create a new 1-to-1 mapping:
  - Interface - WAN
  - Address Family - IPv4
  - External subnet IP → WAN address
  - Internal IP → LAN Net
  - Destination - Any

### VLANs

VLANs, or virtual LANs, is a feature of switches that allow you to logically separate broadcast domains on a single switch.

Simply put, with a standard switch without VLANs, you can only attach one network onto that switch. If you want to have two separate networks, you would need two separate switches.

<p align="center">
  <img src="../assets/images/red-vs-blue/rvb-networking-no-vlan.png" />
</p>

Each network would need to be physicaly separated with two different physical switches. However, with VLANs, you can attach multiple, logically separateed networks on a single switch.

<p align="center">
  <img src="../assets/images/red-vs-blue/rvb-networking-with-vlan.png" />
</p>

In this example, notice how the number of switches have reduced with the implentation of VLANS. If we wanted to create a three, four, even a hundred separate networks, we can utilize VLANs to do all of that on a single switch.

That's one of the benefits of VLANs - less infrastructure overhead when creating new networks, and this is the main benefit we'll focus on in this blog.

Now how does this relate to vSwitches and ESXi networking? If you recall from two sections ago:
> vSwitches operate pretty much exactly the same as physical switches.

If we wanted to create separate, "physically" isolated networks on our ESXi instances, then we would need to create separate vSwitches for each network. However, ESXi comes with the amazing ability to configure VLANs on our vSwitches.

If we go back to our production/development/engineering port group example, we can configure VLANs on each port group:
- Production Port Group - VLAN 10
- Development Port Group - VLAN 20
- Engineering Port Group - VLAN 30

And this would have the effect of isolating each port group so that the Production VMs are on a separate network from the Development VMs, the Development VMs separate from the Engineering VMs, so on and so forth.

Now with VLANs in mind, let's go back to RvB networking.

If we refer back to our first RvB topology, you can see that each pod network would require its own vSwitch. The diagram only shows three pod networks, but our RvB can get upwards to 20 teams, which means having to create 20 vSwitches.

This is a lot of unnecessary work and would create a mess of vSwitches and port groups in our ESXi lab.

I wanted to reduce creating new vSwitches whenever possible, so here were the steps I did:
- Remove all the unnecessary vSwitches and just make one vSwitch that will hold all of our networks.
- Create four port groups on the single vSwitch:
  - CoreNetwork
  - Pod01Network
  - Pod02Network
  - Pod03Network
- Configure the CoreNetwork to have VLAN 1000,
- Configure Pod01Network to have VLAN 1001,
- Configure Pod02Network to have VLAN 1002,
- Configure Pod03Network to have VLAN 1003,
- etc.

(by the way, I did not need to configure these VLANs with these exact VLAN IDs. I could have made CoreNetwork to be VLAN 50 and Pod01Network to be VLAN 635 and this would still operate the same)

To further explain how this works, let's look closely at a single pod network with our new VLAN configuration:

<p align="center">
  <img src="../assets/images/red-vs-blue/rvb-networking-with-vlan-explained1.png" />
</p>

This is what a single pod network would look like with the new VLAN configuration. If we take away the VLAN configurations but retain our same color-coding scheme, this is what the network would look like:

<p align="center">
  <img src="../assets/images/red-vs-blue/rvb-networking-with-vlan-explained2.png" />
</p>

As you can see, by "unwrapping" our VLAN configuration from our network, our topology becomes much more simplier to understand. Here's another view that shows the path of an outbound connection from Pod 1's VM to the Core Router:

<p align="center">
  <img src="../assets/images/red-vs-blue/rvb-networking-with-vlan-explained3.png" />
</p>

In other words, the interactions and communications between the VMs remain the same and networking works the same as the previous topology. However, with VLANs, we can reduce the number of vSwitches created, reducing our networking overhead on our ESXi instance.

### vSphere clustering and dSwitches

We also had an upgrade to our lab - instead of having individuals ESXi machines, we created an ESXi cluster with vMotion and iSCSI configured to allow for load balancing and other cool VM stuff. However, instead of using vSwitches, we had to use distributed switches, or dSwitches for short.

vSwitches and dSwitches do pretty much the same thing, except vSwitches are unique per ESXi, while dSwitches exist for all ESXi's in a cluster.

The way that I visualize dSwitches is as a very wide switch that spans across all of our ESXi machines:



When we configured dSwitches initially, we were prompted for many other options, like uplinks, which I wasn't really familiar with so I just skipped those steps. After creating our dSwitch and our port groups with unique VLAN IDs, we ran into another issue. The issue was that machines on different ESXi's weren't able to communicate with each other. After much troubleshooting, I figured out that the issue was because the dSwitches weren't configured with uplinks. Uplinks allowed the dSwitch to operate across multiple ESXis. Without the uplink, the dSwitch would still exist across the ESXis (you can still add port groups and connect VMs to those port groups) but each ESXi would treat the dSwitch as their own local vSwitch. This means that VMs on the same ESXi on the same dSwitch can communicate with each other, but VMs on different ESXis but on the same dSwitch can't communicate with each other.

The way that I was able to solve this issue was to create the uplinks. But how? This was how:

In my setup, the uplinks were attached to each ESXi's vmnic0, which was configured as a trunk port. Each of the vmnic0's were connected to switchports on some HP switch that were configured as trunk VLANs with VLANs 1000-2000 being allowed through the ports. These VLANs only existed on the switch level; no routers contained any configurations for VLANs 1000-2000.

This would allow a user to create a port group with a unique VLAN ID, and be able to have VMs on that port group communicate with each other regardless of which ESXi the VM belonged on. Things like creating VMs, vMotion, and load balancing cause VMs to be automatically placed and migrated in different ESXi's, making this configuration an ideal way to abstract networking on the individual ESXi level.

### Replacing the core router

After reviewing the topology, I noticed that having a separate pfSense VM as a core router is repetitive. The core router's purpose was to create a 172.16.0.0/16 network. We can just make a new VLAN on the physical router with the 172.16.0.0/16 network and have that VLAN also be routed through the ESXis.

Also, the red team machines and the scoring engine are now placed on the 172.16.0.0/16 network. There's really no reason to have a separate red team subnet. The less networks, the better.

### Final thoughts

When viewing the previous RvB setup, the networking was a lot more simpler and easy to understand. VLANs made sense as well. However, when vSphere clustering and dSwitches became involved, traditional networking convention was thrown out the window. Having to understand the function of dSwitch uplinks, implemeting VLANs into vmnics and dSwitches, and how to figure out how to bridge the connection between VMware networking and physical networking was the biggest hurdle for me.