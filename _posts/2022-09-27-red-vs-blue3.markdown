---
title: Red vs. Blue - Networking and vSphere Explained
layout: post
date: 2022-09-27 22:01
image: /assets/images/markdown.jpg
headerImage: false
tag:
- rvb
category: blog
author: taylornguyen
description: The final boss of Red vs. BLue
---

Hello! Welcome back to my third installment of Red vs. Blue content. This post will aim to explain the concepts and developments of the networking infrastructure behind our Red vs. Blue project. **Many of the concepts here will build off the Networking section in [my first Red vs. Blue post](https://tsnguyen.com/red-vs-blue/), so please check it out if you haven't seen it yet!**

---

### Quick refresher on Red vs. Blue

Red vs. Blue (RvB) is our college organization's twist on the CCDC format. Teams of 4 are placed into a vulnerable environment and given a unique experience of troubleshooting, responding to incidents, and completing business objectives against an active red team.

Preparation and development of an RvB-style competition involves few major steps: brainstorming the setting, creating vulnerable machines, configuring the networking, and setting up a scoring engine.

To read more about RvB, check out this blog by [CPP SWIFT](https://www.calpolyswift.org/blog/rvb-2022-spring).

### Previous Red vs. Blue networking configuration

This was how RvB was previously setup. After the previous networking guy graduated, this was basically what I was left with in terms of documentation.

![Topology](../assets/images/red-vs-blue/RvB-Topology.png)

### Primer to ESXi and vSphere networking

We run RvB mostly off ESXi, which is a Type-1 hypervisor made by VMware. In addition to creating VMs, you can create virtual networks to connect VMs together.

- VMware ESXi allows you to create **vSwitches** (virtual switches).
- vSwitches operate pretty much exactly the same as physical switches.
- vSwitches contain **port groups**, or groups of virtual ports on a vSwitch.

To understand port groups a bit better, imagine you have a super huge, unconfigured virtual switch that contains an absurdly large number of ports.

Chances are, you don't want all the ports to be configured exactly the same way -- you want some ports to be used by production VMs, some ports to be used by development VMs, some ports to be used by engineering VMs, etc.

We can make a port group for each of these:
- a port group for production VMs,
- a port group for development VMs,
- a port group for engineering VMs,
- and so on and so forth.

We can then create logical rules or policies on each port group. For example, if we want to implement traffic shaping on our production VMs, we can do so by editing the traffic policies on the production VMs' port group without needing to affect the development VMs and engineering VMs.

ESXi port groups are a totally different concept than things like port bonding, port aggregation, etc. If you want to learn more, [this article](https://vmiss.net/vmware-vsphere-networking/) has a good read on vSphere networking.

### 1-to-1 NAT

The first thing I focused on was reimplementing the 1-to-1 NAT on the pod pfSense routers. Every router needs to have some special gimmick to make it unique against other brands, so I had to consult the [pfSense documentation](https://docs.netgate.com/pfsense/en/latest/nat/1-1.html) to figure out what was going on.

Anyways, the goal of 1-to-1 NAT is to get every system on each pod to be individually accessible from the core network (AKA have its own IP address in the core network subnet range).

![1-to-1 NAT](../assets/images/red-vs-blue/rvb-networking-nat.png)

**Pod Router 1 NAT Mappings**
- 192.168.1.10 --> 172.16.1.10
- 192.168.1.11 --> 172.16.1.11
- 192.168.1.12 --> 172.16.1.12

**Pod Router 2 NAT Mappings**
- 192.168.1.10 --> 172.16.1.10
- 192.168.1.11 --> 172.16.1.11
- 192.168.1.12 --> 172.16.1.12

**Pod Router 3 NAT Mappings**
- 192.168.1.10 --> 172.16.1.10
- 192.168.1.11 --> 172.16.1.11
- 192.168.1.12 --> 172.16.1.12

After much trial and error, this was the NAT configuration that allowed us to achieve 1-to-1 NAT:

- Under System → Routing, ensure that the default gateway is configured to be the Core Router's LAN interface (172.16.0.1).
- Under Firewall → Virtual IPs, create a new entry with the following settings:
  - Type - Proxy ARP
  - Interface - WAN
  - Address Type - Network
  - Address(es) - 172.16.X.0/24 (X being the team number)
- Under Firewall → NAT, create a new 1:1 mapping:
  - Interface - WAN
  - Address Family - IPv4
  - External subnet IP → WAN address
  - Internal IP → LAN Net
  - Destination - Any

### VLANs

VLANs, or virtual LANs, is a feature of switches that allow you to logically separate broadcast domains on a single switch.

Simply put, with a standard switch without VLANs, you can only attach one network onto that switch. If you want to have two networks, you would need two switches.

With VLANs, you can attach multiple networks on a single switch.

In this example, notice how we reduced the number of switches when we implemented VLANs. If I create new VLANs, I can add many more networks onto that one single switch! That's one of the benefits of VLANs - less infrastructure overhead when creating new networks.

Now with VLANs in mind, let's go back to RvB networking.

One of the issues with the current setup of RvB is having too many vSwitches – each pod would require its own vSwitch, which is kind of unnecessary since I would have to create an entirely new vSwitch with a new port group for each team. Our lab eventually became a mess of vSwitches and port groups, I wanted to reduce creating new vSwitches whenever possible. So I configured each RvB pod's port group to belong in a different VLAN. These VLANs only existed within the port groups - the VLANs were not configured on any physical switch or router. This would make it so that each pod is isolated into their own separate broadcast domain, but still be connected to one vSwitch. Therefore, we can still create pod networks without having to create additional vSwitches.

### vSphere clustering and dSwitches

We also had an upgrade to our lab - instead of having individuals ESXi machines, we created an ESXi cluster with vMotion and iSCSI configured to allow for load balancing and other cool VM stuff. However, instead of using vSwitches, we had to use distributed switches, or dSwitches for short. vSwitches and dSwitches do pretty much the same thing, except vSwitches are unique per ESXi, while dSwitches exist for all ESXi's in a cluster.

I like to draw dSwitches as a very wide switch.

When we configured dSwitches initially, we were prompted for many other options, like uplinks, which I wasn't really familiar with so I just skipped those steps. After creating our dSwitch and our port groups with unique VLAN IDs, we ran into another issue. The issue was that machines on different ESXi's weren't able to communicate with each other. After much troubleshooting, I figured out that the issue was because the dSwitches weren't configured with uplinks. Uplinks allowed the dSwitch to operate across multiple ESXis. Without the uplink, the dSwitch would still exist across the ESXis (you can still add port groups and connect VMs to those port groups) but each ESXi would treat the dSwitch as their own local vSwitch. This means that VMs on the same ESXi on the same dSwitch can communicate with each other, but VMs on different ESXis but on the same dSwitch can't communicate with each other.

The way that I was able to solve this issue was to create the uplinks. But how? This was how:

In my setup, the uplinks were attached to each ESXi's vmnic0, which was configured as a trunk port. Each of the vmnic0's were connected to switchports on some HP switch that were configured as trunk VLANs with VLANs 1000-2000 being allowed through the ports. These VLANs only existed on the switch level; no routers contained any configurations for VLANs 1000-2000.

This would allow a user to create a port group with a unique VLAN ID, and be able to have VMs on that port group communicate with each other regardless of which ESXi the VM belonged on. Things like creating VMs, vMotion, and load balancing cause VMs to be automatically placed and migrated in different ESXi's, making this configuration an ideal way to abstract networking on the individual ESXi level.

### Replacing the core router

After reviewing the topology, I noticed that having a separate pfSense VM as a core router is repetitive. The core router's purpose was to create a 172.16.0.0/16 network. We can just make a new VLAN on the physical router with the 172.16.0.0/16 network and have that VLAN also be routed through the ESXis.

Also, the red team machines and the scoring engine are now placed on the 172.16.0.0/16 network. There's really no reason to have a separate red team subnet. The less networks, the better.

### Final thoughts

When viewing the previous RvB setup, the networking was a lot more simpler and easy to understand. VLANs made sense as well. However, when vSphere clustering and dSwitches became involved, traditional networking convention was thrown out the window. Having to understand the function of dSwitch uplinks, implemeting VLANs into vmnics and dSwitches, and how to figure out how to bridge the connection between VMware networking and physical networking was the biggest hurdle for me.